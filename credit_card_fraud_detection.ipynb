{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6037e22",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7730a82",
   "metadata": {},
   "source": [
    "## Libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1323ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "\n",
    "# Standardization\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Divisão treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from yellowbrick.classifier.rocauc import roc_auc\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Hyperparameter selection\n",
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c326959",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraude = pd.read_csv('creditcard_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13efc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66382e2",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40394b67",
   "metadata": {},
   "source": [
    "The objective is to know and explore the data, identifying patterns/relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be597a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraude.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e99f12",
   "metadata": {},
   "source": [
    "There are 30 variables in the dataset, all numeric:\n",
    "- Features V1-V28: principal components obtained with PCA (due to confidentiality issues).\n",
    "- Time: contains the seconds elapsed between each transaction and the first transaction in the dataset.\n",
    "- Amount: is the transaction Amount\n",
    "- Class: is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values. # There are no missing values\n",
    "data_fraude.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45754f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data that has been classified as fraud\n",
    "data_fraude[data_fraude['Class'] ==1][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96b327",
   "metadata": {},
   "source": [
    "There are many values of Amount 1 (small financial values) in the Fraud class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "pd.set_option('display.max_columns', None) # see all columns\n",
    "data_fraude.describe(percentiles = [.25, .5, .75, .90, .95, .99]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11b34e",
   "metadata": {},
   "source": [
    "As informed in Kaggle, for security reasons, the real variables are not being shared, they were transformed by PCA. Thus, the variables do not have much discrepancy, with the exception of the Amount and Time variables, which were not transformed by PCA.\n",
    "\n",
    "The Amount variable has a mean of 88. The mean here is not a good metric to look at this data, since we have a median of 22 and Quartile 3 of 77, that is, 75% of the values are below the mean. The average is being influenced by high values (outliers) of amount. This can also be seen in the standard deviation, which is 250.\n",
    "\n",
    "Let's look at these metrics by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for the graphics\n",
    "cores = ['#436DA9', '#E73788']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78304e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of variable class with amount\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(data=data_fraude, x='Class', y='Amount', palette=cores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of variable class with amount\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(x = 'Class', y = 'Amount', data = data_fraude[data_fraude.Amount < 1e2], palette=cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d91845",
   "metadata": {},
   "source": [
    "The graphs above corroborate the conclusions of the statistical summary of the data. Most of the Amount values are below 100, but we have outliers, which are as high as 25,000 in the Non-Fraud Class and are generating a mean much higher than the median of the data, as well as a high standard deviation.\n",
    "\n",
    "In the case of non-fraud, we have more data between the second and third quartile, which varies from 10 to 40 reais. In the case of fraud, most data are between the second and third quartile, ranging from 1 to 25 reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of amount by class\n",
    "fraude = data_fraude[data_fraude['Class'] == 1]\n",
    "naofraude = data_fraude[data_fraude['Class'] == 0]\n",
    "\n",
    "print(\"Fraude - resumo estatístico\")\n",
    "print(fraude[\"Amount\"].describe())\n",
    "print(\"\\nNão Fraude - resumo estatístico\")\n",
    "print(naofraude[\"Amount\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c0eae",
   "metadata": {},
   "source": [
    "As seen in the boxplots above, both categories have a similar mean value, but it does not represent the data well, as 75% of them have a value below the mean. Both categories have outliers, but non-fraudulent transactions have outliers with much higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eee603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time variable is the simulation moment (2 days), in which the transaction is performed. Distribution throughout the day\n",
    "\n",
    "# Converting Time to hours\n",
    "timedelta = pd.to_timedelta(data_fraude['Time'], unit='s')\n",
    "data_fraude['Time_h'] = (timedelta.dt.components.hours).astype(int)\n",
    "\n",
    "data_fraude.drop(['Time'], axis=1, inplace=True)\n",
    "\n",
    "# Plotting the 24-hour transaction graph\n",
    "bins=24\n",
    "data_fraude[(data_fraude['Class'] == 0)].hist(column=\"Time_h\",color=\"#436DA9\",bins=bins)\n",
    "data_fraude[(data_fraude['Class'] == 1)].hist(column =\"Time_h\",color=\"#E73788\",bins=bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a better view of how transactions occur throughout the day\n",
    "plt.figure(figsize=(12,6))\n",
    "target_0 = data_fraude.loc[data_fraude['Class'] == 0]\n",
    "target_1 = data_fraude.loc[data_fraude['Class'] == 1]\n",
    "\n",
    "\n",
    "sns.distplot(target_0[['Time_h']], hist=False, rug=True, color=\"#436DA9\",)\n",
    "sns.distplot(target_1[['Time_h']], hist=False, rug=True, color=\"#E73788\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d28d60",
   "metadata": {},
   "source": [
    "Fraudulent transactions have a more even distribution, they are evenly distributed over 24 hours. Normal transactions are smaller at night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69179502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking duplicate lines\n",
    "data_fraude.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e66dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraude.drop_duplicates(inplace=True) # deleting duplicate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f09274",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraude.shape # There were approximately 1000 duplicate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d27879",
   "metadata": {},
   "source": [
    "The problem is a case of supervised learning of binary classification, where we use labeled data to train the model, the data contains the desired answer (fraud or non-fraud). Let's analyze the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb31497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of the target class\n",
    "plt.figure(figsize=(8,10))\n",
    "g = sns.countplot('Class', data=data_fraude, palette=cores)\n",
    "g.set_title('Distribuição das Classes')\n",
    "g.set_ylabel('Quantidade de ocorrências')\n",
    "g.set_xlabel('Classe')\n",
    "g.set_xticklabels(['Não Fraude', 'Fraude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fec198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Fraud Percentage\n",
    "total = data_fraude['Class'].value_counts()[0] + data_fraude['Class'].value_counts()[1]\n",
    "fraude = (data_fraude['Class'].value_counts()[1]/total) * 100\n",
    "print('Porcentagem de fraude:', fraude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2964912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data in numbers\n",
    "total = len(data_fraude)\n",
    "normal = len(data_fraude[data_fraude.Class == 0])\n",
    "fraudes = len(data_fraude[data_fraude.Class == 1])\n",
    "print('Número total de transações {}'.format(total))\n",
    "print('Número de Transações Normais {}'.format(normal))\n",
    "print('Número de Transações Fraudulentas {}'.format(fraudes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495579d1",
   "metadata": {},
   "source": [
    "The target class has two outputs: non-fraud (0) and fraud (1). As expected in banking transactions, fraud cases are much lower than normal transactions. Here, frauds represent only 0.166% of the data (473 out of 280922) which makes the target variable very unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae0971a",
   "metadata": {},
   "source": [
    "#### Let's check the financial impact of fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5039bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount transacted in 2 days\n",
    "sum(data_fraude['Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17619f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of fraud\n",
    "fraude = data_fraude[data_fraude[\"Class\"] == 1]\n",
    "sum(fraude['Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "naofraude = data_fraude[data_fraude[\"Class\"] == 0]\n",
    "sum(naofraude['Amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd0d0d",
   "metadata": {},
   "source": [
    "We handled an amount of R$ 25 billion in 2 days, of which R$ 58,000 were from fraudulent transactions, which represents 0.23% of financial loss with customer reimbursement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823d8d2",
   "metadata": {},
   "source": [
    "## Seleção de features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e086e",
   "metadata": {},
   "source": [
    "We will select the most important features to reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f509579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "d = data_fraude.iloc[:,:29]\n",
    "e = data_fraude.iloc[:,30:]\n",
    "data = pd.concat([d, e], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1658c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alvo = pd.DataFrame(data_fraude.iloc[:,29])\n",
    "alvo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f64cd",
   "metadata": {},
   "source": [
    "As we don't know what the features are (they went through PCA), let's make a selection of the ones that most contribute to the prediction variable, through Mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29160098",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = mutual_info_classif(data, alvo)\n",
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating series to better visualize the most important variables\n",
    "mi = pd.Series(mi)\n",
    "mi.index = data.columns\n",
    "mi = mi.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "mi.plot.bar(figsize=(22,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the most important features\n",
    "sel = SelectKBest(mutual_info_classif, k=10).fit(data, alvo)\n",
    "data.columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef1b56",
   "metadata": {},
   "source": [
    "We chose the top 10 variables to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selecionadas = data_fraude[['V3', 'V4', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18', 'Time_h', 'Class']]\n",
    "selecionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5cb98",
   "metadata": {},
   "source": [
    "## Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = selecionadas.drop(columns = ['Class'])\n",
    "y = selecionadas['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "# Stratify to ensure both groups contain the same percentage of fraud cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6d0c4",
   "metadata": {},
   "source": [
    "We split the data by putting 70% data in “training set” and 30% data in “testing set”, to avoid overfitting. If we use the same data to test the model that was used for training, then the model will perform well, but this is not good as the model memorizes the data and will not provide accurate results for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e04431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset size\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7761e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset size\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c862e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the percentage of classes in datasets\n",
    "print('Não fraude', round(\n",
    "        y_train.value_counts()[0]/len(y_train)*100, 2), '% do dataset de treino')\n",
    "print('Fraude', round(\n",
    "        y_train.value_counts()[1]/len(y_train)*100, 2), '% do dataset de treino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Não fraude', round(\n",
    "        y_test.value_counts()[0]/len(y_test)*100, 2), '% do dataset de teste')\n",
    "print('Fraud', round(\n",
    "        y_test.value_counts()[1]/len(y_test)*100, 2), '% do dataset de teste')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d8c97",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785cd851",
   "metadata": {},
   "source": [
    "Let's standardize the data to put it into a common range of values as they can impact model metrics. The model will be able to \"learn\" that larger values have greater relevance for the forecast, but will have this conclusion under the influence of the column order of magnitude, and not by the importance of the variable itself. We avoid this with the standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceceecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16978016",
   "metadata": {},
   "source": [
    "## Random Forest for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(random_state=0)\n",
    "\n",
    "RF.fit(X_train_scaled, y_train)\n",
    "\n",
    "predictions = RF.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35875a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, RF.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = ConfusionMatrix(RF, classes=['Não Fraude', 'Fraude'], cmap=['#436DA9', '#E73788'])\n",
    "matrix.fit(X_train_scaled, y_train)\n",
    "matrix.score(X_test_scaled, y_test)\n",
    "matrix.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the classes are unbalanced, the AP curve will be considered for the evaluation of the model\n",
    "display2 = PrecisionRecallDisplay.from_estimator(RF, X_test_scaled, y_test, name=\"Random Forest teste\", color='#436DA9')\n",
    "display2 = PrecisionRecallDisplay.from_estimator(RF, X_train_scaled, y_train, name=\"Random Forest treino\", color='#E73788', ax=display2.ax_ )\n",
    "display2.figure_.suptitle(\"Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just looking at the ROC curve out of curiosity.\n",
    "rff = metrics.plot_roc_curve(RF, X_test_scaled, y_test, name='Random Forest teste', color='#436DA9')\n",
    "rff = metrics.plot_roc_curve(RF, X_train_scaled, y_train, name='Random Forest treino', color='#E73788', ax=rff.ax_)\n",
    "rff.figure_.suptitle(\"Random Forest\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e71ab",
   "metadata": {},
   "source": [
    "We had good results, so let's cross-validate for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c86d2",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation of recall\n",
    "recallRF = cross_val_score(RF, X_train_scaled, y_train, cv=5, scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11076cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recallRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recallRF.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ba670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recallRF.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d09865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervalo(recallRF):\n",
    "    mean = recallRF.mean()\n",
    "    dv = recallRF.std()\n",
    "    print('Recall média: {:.2f}%'.format(mean*100))\n",
    "    print('Intervalo de recall: [{:.2f}% ~ {:.2f}%]'\n",
    "           .format((mean - 2*dv)*100, (mean + 2*dv)*100))\n",
    "intervalo(recallRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b1d93",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91816d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking parameters RandomForest\n",
    "?RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0dcb83",
   "metadata": {},
   "source": [
    "Let's try to improve the model. For this, we are going to use some Random Forest hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947b741",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406352b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which are the best hyperparameters that improve the AP curve\n",
    "\n",
    "def treinar_modelo(params):\n",
    "    max_features = params[0]\n",
    "    n_estimators = params[1]\n",
    "    max_depth = params[2]\n",
    "\n",
    "    \n",
    "    print(params, '\\n') # prints which parameters are testing on each iteration\n",
    "    \n",
    "    mdl = RandomForestClassifier(max_features=max_features, n_estimators=n_estimators,\n",
    "                        max_depth=max_depth, random_state=0, n_jobs=-1)\n",
    "    mdl.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    predictions = mdl.predict(X_test_scaled)\n",
    "    \n",
    "    return -average_precision_score(y_test, predictions) \n",
    "# The returned metric will be the one chosen to check if the model is good due to imbalance\n",
    "\n",
    "# Minimum and maximum parameter values\n",
    "space = [(0.1, 1.0), #max_features\n",
    "         (100, 500), # n_estimators\n",
    "         (1, 8)] # max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_gp = gp_minimize(treinar_modelo, space, \n",
    "                            random_state=1, \n",
    "                            verbose=1, #show process\n",
    "                            n_calls=30, # number of interactions it will test\n",
    "                            n_random_starts=10) #testa amostras de 10 pontos aleatoriamente, depois treina o modelo e encontra \n",
    "# qual parâmetro foi mais promissor, e a partir disso testa parâmetros que parecem mais optimos e explora eles ao invés\n",
    "# de fazer isso aleatoriamente. Explorar áreas promissoras e explorar áreas que parecem melhores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16d7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the best hyperparameters\n",
    "resultados_gp.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd63f5",
   "metadata": {},
   "source": [
    "## Running the model with the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF2 = RandomForestClassifier(random_state=0, max_features=0.55, n_estimators=498, max_depth=7)\n",
    "\n",
    "RF2.fit(X_train_scaled, y_train)\n",
    "\n",
    "predictions2 = RF2.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5de141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions2))\n",
    "print(classification_report(y_test, RF.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5708d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display2 = PrecisionRecallDisplay.from_estimator(RF2, X_test_scaled, y_test, name=\"Random Forest teste\", color='#436DA9')\n",
    "display2 = PrecisionRecallDisplay.from_estimator(RF2, X_train_scaled, y_train, name=\"Random Forest treino\", color='#E73788', ax=display2.ax_ )\n",
    "display2.figure_.suptitle(\"Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a5d1e2",
   "metadata": {},
   "source": [
    "Even looking for better hyperparameters, the model didn't have many gains. However, the end result is considered good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11726d",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b515ef2a",
   "metadata": {},
   "source": [
    "- The model correctly classified 84131 non-fraud transactions\n",
    "- The model classified only 4 true transactions as fraud (here we have the lowest number of customers with transactions being barred, as our objective was also to avoid classifying good customers as suspicious as much as possible)\n",
    "- The model correctly classified 115 frauds\n",
    "- The model classified 27 frauds as true transactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
